https://www.tensorflow.org/versions/master/tutorials/image_recognition



resos:
  http://colah.github.io/posts/2014-07-Conv-Nets-Modular/
  http://neuralnetworksanddeeplearning.com/chap6.html
  

Researchers have demonstrated steady progress in computer vision by validating their work against ImageNet -- an academic benchmark for
computer vision. Successive models continue to show improvements, each time achieving a new state-of-the-art result: QuocNet, AlexNet,
Inception (GoogLeNet), BN-Inception-v2. 

Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer 
vision, where models try to classify entire images into 1000 classes, like "Zebra", "Dalmatian", and "Dishwasher". \
To compare models, we examine how often the model fails to predict the correct answer as one of their top 5 guesses -- termed "top-5 error 
rate".

AlexNet achieved by setting a top-5 error rate of 15.3% on the 2012 validation data set; Inception (GoogLeNet) achieved 6.67%; BN-
Inception-v2 achieved 4.9%; Inception-v3 reaches 3.46%.

 (http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/ 
 Andrej Karpathy who attempted to measure his own performance. He reached 5.1% top-5 error rate.)
 
 This tutorial will teach you how to use Inception-v3. You'll learn how to classify images into 1000 classes in Python or C++. We'll also 
 discuss how to extract higher level features from this model which may be reused for other vision tasks.


classify_image.py downloads the trained model from tensorflow.org when the program is run for the first time.
If you wish to supply other JPEG images, you may do so by editing the --image_file argument.
If you download the model data to a different directory, you will need to point --model_dir to the directory used.







https://www.tensorflow.org/versions/master/tutorials/image_retraining
resos: https://arxiv.org/pdf/1310.1531v1.pdf

Modern object recognition models have millions of parameters and can take weeks to fully train. Transfer learning is a technique that 
shortcuts a lot of this work by taking a fully-trained model for a set of categories like ImageNet, and retrains from the existing 
weights for new classes. 
Though it's not as good as a full training run, this is surprisingly effective for many applications, and can be run in as little as 
thirty minutes on a laptop, without requiring a GPU.


Before you start any training, you'll need a set of images to teach the network about the new classes you want to recognize.
    cd ~
    curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
    tar xzf flower_photos.tgz

from the root of your TensorFlow source directory:
bazel build tensorflow/examples/image_retraining:retrain
(if you have a machine which supports the AVX instruction set (common in x86 CPUs produced in the last few years) you can improve the 
running speed
The retrainer can then be run like this:
bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos

This script loads the pre-trained Inception v3 model, removes the old top layer, and trains a new one on the flower photos you've 
downloaded. None of the flower species were in the original ImageNet classes the full network was trained on. The magic of transfer 
learning is that lower layers that have been trained to distinguish between some objects can be reused for many recognition tasks 
without any alteration.
)





Drawbacks (Bottlenecks):
The script can take thirty minutes or more to complete, depending on the speed of your machine. The first phase analyzes all the images 
on disk and calculates the bottleneck values for each of them. 

'Bottleneck' is an informal term we often use for the layer just before 
the final output layer that actually does the classification.


This penultimate layer has been trained to output a set of values that's good enough for the classifier to use to distinguish between 
all the classes it's been asked to recognize. That means it has to be a meaningful and compact summary of the images, since it has to 
contain enough information for the classifier to make a good choice in a very small set of values.

The reason our final layer retraining 
can work on new classes is that it turns out the kind of information needed to distinguish between all the 1,000 classes in ImageNet is 
often also useful to distinguish between new kinds of objects.

Because every image is reused multiple times during training and calculating each bottleneck takes a significant amount of time, it 
speeds things up to cache these bottleneck values on disk so they don't have to be repeatedly recalculated. By default they're stored in 
the /tmp/bottleneck directory, and if you rerun the script they'll be reused so you don't have to wait for this part again.






Training:
  Once the bottlenecks(penultimate layers) are complete, the actual training of the top layer(final layer) of the network begins. 
   You'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy. 
    
    The training accuracy shows what percent of the images used in the current training batch were labeled with the correct class. 
    
    The validation accuracy is the precision on a randomly-selected group of images from a different set. 
    
    The key difference is that the training accuracy is based on images that the network has been able to learn from so the network can 
    overfit to the noise in the training data. A true measure of the performance of the network is to measure its performance on a data 
    set not contained in the training data -- this is measured by the validation accuracy.
    
    If the train accuracy is high but the validation accuracy remains low, that means the network is overfitting and memorizing 
    particular features in the training images that aren't helpful more generally. (nice! :) )
    
    Cross entropy is a loss function which gives a glimpse into how well the learning process is progressing. The training's objective 
    is to make the loss as small as possible, so you can tell if the learning is working by keeping an eye on whether the loss keeps 
    trending downwards, ignoring the short-term noise.
    
    
    
    
    By default this script will run 4,000 training steps. Each step chooses ten images at random from the training set, finds their 
    bottlenecks from the cache, and feeds them into the final layer to get predictions. Those predictions are then compared against the 
    actual labels to update the final layer's weights through the back-propagation process. 
    
    As the process continues you should see the reported accuracy improve, and after all the steps are done, a final test accuracy 
evaluation is run on a set of images kept separate from the training and validation pictures. This test evaluation is the best estimate 
of how the trained model will perform on the classification task. You should see an accuracy value of between 90% and 95%, though the 
exact value will vary from run to run since there's randomness in the training process. 

This number is based on the percent of the images in the test set that are given the correct label after the model is fully trained.
  

  '''Visualizing with TensorBoard:'''
     you can visualize the graph and statistics, such as how the weights or accuracy varied during training.
    To launch TensorBoard, run this command during or after retraining:
      tensorboard --logdir /tmp/retrain_logs
      
      Once TensorBoard is running, navigate your web browser to localhost:6006 to view the TensorBoard.
The script will log TensorBoard summaries to /tmp/retrain_logs by default. You can change the directory with the --summaries_dir flag.
  (read this for more tips and tricks: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md)
